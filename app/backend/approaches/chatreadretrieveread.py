import openai
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType
from approaches.approach import Approach
from text import nonewlines

# Simple retrieve-then-read implementation, using the Cognitive Search and OpenAI APIs directly. It first retrieves
# top documents from search, then constructs a prompt with them, and then uses OpenAI to generate an completion 
# (answer) with that prompt.
class ChatReadRetrieveReadApproach(Approach):
    prompt_prefix = """<|im_start|>system
El asistente ayuda a los empleados de la empresa con preguntas sobre información de los diferentes procesos de la telefónica Claro registrados en Claropedia. 
Responda SOLO con los hechos enumerados en la lista de fuentes a continuación. Si no hay suficiente información a continuación, diga que no lo sabe. No genere respuestas que no utilicen las fuentes a continuación. Si hacer una pregunta aclaratoria al usuario ayudaría, haga la pregunta. 
Cada fuente tiene un nombre seguido de dos puntos y la información real, siempre incluya el nombre de la fuente para cada hecho que use en la respuesta. Use corchetes para hacer referencia a la fuente, por ejemplo [info1.txt]. No combine fuentes, liste cada fuente por separado, por ejemplo [info1.txt][info2.txt].
{follow_up_questions_prompt}
{injected_prompt}
Sources:
{sources}
<|im_end|>
{chat_history}
"""

    follow_up_questions_prompt_content = """Genere tres preguntas de seguimiento muy breves que el usuario probablemente haría a continuación sobre su plan de atención, el manual del empleado, sobre venta de equipos, devoluciones, gestión de deudas, procesamiento de pedidos, facturación, compras, anulaciones, etc.
     Utilice corchetes angulares dobles para hacer referencia a las preguntas, p. <<¿Cuál es la documentación necesaria por venta de equipo a corporativos?>>.
     Trate de no repetir preguntas que ya se han hecho.
     Solo genere preguntas y no genere ningún texto antes o después de las preguntas, como 'Próximas preguntas'"""

    query_prompt_template = """A continuación se muestra un historial de la conversación hasta el momento y una nueva pregunta hecha por el usuario que debe responderse buscando en la base de conocimientos sobre la información de los diferentes procesos internos de la compañía Claro.
     Genere una consulta de búsqueda basada en la conversación y la nueva pregunta.
     No incluya los nombres de los archivos fuente ni los nombres de los documentos citados, por ejemplo, info.txt o doc.pdf en los términos de la consulta de búsqueda. No incluya ningún texto entre [] o <<>> en los términos de consulta de búsqueda.
     Si la pregunta no está en español, traduzca la pregunta al español antes de generar la consulta de búsqueda.

Historial de chat:
{chat_history}

Pregunta:
{question}

Consulta de busqueda:
"""

    def __init__(self, search_client: SearchClient, chatgpt_deployment: str, gpt_deployment: str, sourcepage_field: str, content_field: str):
        self.search_client = search_client
        self.chatgpt_deployment = chatgpt_deployment
        self.gpt_deployment = gpt_deployment
        self.sourcepage_field = sourcepage_field
        self.content_field = content_field

    def run(self, history: list[dict], overrides: dict) -> any:
        use_semantic_captions = True if overrides.get("semantic_captions") else False
        top = overrides.get("top") or 3
        exclude_category = overrides.get("exclude_category") or None
        filter = "category ne '{}'".format(exclude_category.replace("'", "''")) if exclude_category else None

        # STEP 1: Generate an optimized keyword search query based on the chat history and the last question
        prompt = self.query_prompt_template.format(chat_history=self.get_chat_history_as_text(history, include_last_turn=False), question=history[-1]["user"])
        completion = openai.Completion.create(
            engine=self.gpt_deployment, 
            prompt=prompt, 
            temperature=0.0, 
            max_tokens=32, 
            n=1, 
            stop=["\n"])
        q = completion.choices[0].text

        # STEP 2: Retrieve relevant documents from the search index with the GPT optimized query
        if overrides.get("semantic_ranker"):
            r = self.search_client.search(q, 
                                          filter=filter,
                                          query_type=QueryType.SEMANTIC, 
                                          query_language="es-es", 
                                          query_speller="lexicon", 
                                          semantic_configuration_name="default", 
                                          top=top, 
                                          query_caption="extractive|highlight-false" if use_semantic_captions else None)
        else:
            r = self.search_client.search(q, filter=filter, top=top)
        if use_semantic_captions:
            results = [doc[self.sourcepage_field] + ": " + nonewlines(" . ".join([c.text for c in doc['@search.captions']])) for doc in r]
        else:
            results = [doc[self.sourcepage_field] + ": " + nonewlines(doc[self.content_field]) for doc in r]
        content = "\n".join(results)

        follow_up_questions_prompt = self.follow_up_questions_prompt_content if overrides.get("suggest_followup_questions") else ""
        
        # Allow client to replace the entire prompt, or to inject into the exiting prompt using >>>
        prompt_override = overrides.get("prompt_template")
        if prompt_override is None:
            prompt = self.prompt_prefix.format(injected_prompt="", sources=content, chat_history=self.get_chat_history_as_text(history), follow_up_questions_prompt=follow_up_questions_prompt)
        elif prompt_override.startswith(">>>"):
            prompt = self.prompt_prefix.format(injected_prompt=prompt_override[3:] + "\n", sources=content, chat_history=self.get_chat_history_as_text(history), follow_up_questions_prompt=follow_up_questions_prompt)
        else:
            prompt = prompt_override.format(sources=content, chat_history=self.get_chat_history_as_text(history), follow_up_questions_prompt=follow_up_questions_prompt)

        # STEP 3: Generate a contextual and content specific answer using the search results and chat history
        completion = openai.Completion.create(
            engine=self.chatgpt_deployment, 
            prompt=prompt, 
            temperature=overrides.get("temperature") or 0.7, 
            max_tokens=1024, 
            n=1, 
            stop=["<|im_end|>", "<|im_start|>"])

        return {"data_points": results, "answer": completion.choices[0].text, "thoughts": f"Searched for:<br>{q}<br><br>Prompt:<br>" + prompt.replace('\n', '<br>')}
    
    def get_chat_history_as_text(self, history, include_last_turn=True, approx_max_tokens=1000) -> str:
        history_text = ""
        for h in reversed(history if include_last_turn else history[:-1]):
            history_text = """<|im_start|>user""" +"\n" + h["user"] + "\n" + """<|im_end|>""" + "\n" + """<|im_start|>assistant""" + "\n" + (h.get("bot") + """<|im_end|>""" if h.get("bot") else "") + "\n" + history_text
            if len(history_text) > approx_max_tokens*4:
                break    
        return history_text